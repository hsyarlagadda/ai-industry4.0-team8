# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wun6_VSu2Ujwn9DUIR1maSamMFbxn6sn
"""

!pip install openai
!pip install git+https://github.com/huggingface/diffusers transformers accelerate
!pip install gradio

neva_api_url = "https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b"
kosmos2_api_url = "https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2"
fuyu8b_api_url = "https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b"
paligemma_api_url = "https://ai.api.nvidia.com/v1/vlm/google/paligemma"
phi3_api_url = "https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct"

import gradio as gr
import cv2
from PIL import Image
import numpy as np
import io
import base64
import requests
import torch
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.utils import export_to_video
from google.colab import files

# Define the VLM class
class VLM:
    def __init__(self, url, api_key):
        """ Provide NIM API URL and an API key"""
        self.api_key = api_key
        self.url = url
        self.headers = {"Authorization": f"Bearer {self.api_key}", "Accept": "application/json"}

    def _encode_image(self, image):
        """ Resize image, encode as JPEG to shrink size, then convert to Base64 for upload """
        if isinstance(image, np.ndarray):  # OpenCV frame
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)
        elif isinstance(image, Image.Image):  # PIL image
            image = image.convert("RGB")
        else:
            raise ValueError(f"Unsupported image input type: {type(image)}")

        image = image.resize((336, 336))
        buf = io.BytesIO()
        image.save(buf, format="JPEG")
        image_b64 = base64.b64encode(buf.getvalue()).decode()
        return image_b64

    def __call__(self, prompt, image):
        """ Call VLM object with the prompt and a single frame """
        image_b64 = self._encode_image(image)
        payload = {
            "messages": [
                {
                    "role": "user",
                    "content": f'{prompt} Here is the image: <img src="data:image/jpeg;base64,{image_b64}" />'
                }
            ],
            "max_tokens": 128,
            "temperature": 0.20,
            "top_p": 0.70,
            "stream": False
        }

        response = requests.post(self.url, headers=self.headers, json=payload)
        response_json = response.json()
        reply = response_json.get("choices", [{}])[0].get("message", {}).get("content", "")
        return reply

# Instantiate the VLM
api_key = "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"
vlm = VLM("https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b", api_key)

# Define the process video function
def process_video(prompt, video_path):
    cap = cv2.VideoCapture(video_path)  # Open the uploaded video
    frames_processed = 0
    replies = []

    while True:
        ret, frame = cap.read()
        if not ret:  # End of video
            break

        # Process only every 30th frame to reduce computation
        if frames_processed % 30 == 0:
            reply = vlm(prompt, frame)
            replies.append(reply)

        frames_processed += 1
        if frames_processed > 300:  # Limit to first 300 frames
            break
    cap.release()
    return replies

# Load the diffusion pipeline
pipe = DiffusionPipeline.from_pretrained(
    "damo-vilab/text-to-video-ms-1.7b",
    torch_dtype=torch.float16,
    variant="fp16"
)
pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
pipe.enable_vae_slicing()

# Generate video from prompt
def generate_video_from_prompt(prompt):
    # Generate video frames using the diffusion pipeline

# Load the diffusion pipeline
    pipe = DiffusionPipeline.from_pretrained(
        "damo-vilab/text-to-video-ms-1.7b",
        torch_dtype=torch.float16,
        variant="fp16"
    )
    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)

    # Optimize for GPU memory
    pipe.enable_model_cpu_offload()
    pipe.enable_vae_slicing()

    # Define the prompt
    #prompt = "The person in the image is a man. He is smiling."

    # Generate video frames
    video_frames = pipe(prompt, num_inference_steps=30, num_frames=50).frames

    # Ensure frames are in the correct format and handle extra dimension
    formatted_frames = []
    # video_frames is now (1, 200, 256, 256, 3), so we need to iterate over the first dimension
    for batch_frames in video_frames:
        for frame in batch_frames:
            if isinstance(frame, Image.Image):
                frame = frame.convert("RGB")  # Convert to RGB if not already
            elif isinstance(frame, np.ndarray):
                if frame.shape[2] == 4:
                    frame = frame[:, :, :3]  # Remove alpha channel if present
                # Convert the frame to uint8 before creating a PIL Image
                frame = (frame * 255).astype(np.uint8)
                frame = Image.fromarray(frame)  # Convert numpy array to PIL Image
            formatted_frames.append(frame)

    # Export the video frames to a video file
    video_path = "generated_video.mp4"
    export_to_video(formatted_frames, video_path)

    # Print the path to the saved video
    #print(f"Video saved to {video_path}")
    #files.download(video_path)
    return video_path

# Gradio interface function
# Define the Gradio interface function
# Define the Gradio interface function
def gradio_process_input(video):
    prompt = (
        "First tell me if it\'s a man or a woman, and then tell me what is the expression on his/her face so that "
        "I can feed it to another llm. Remember, just give me what I have asked for, don\'t even give greetings. "
        "So what I am expecting from you is something like, \"a man/woman is ...\""
    )
    replies = process_video(prompt, video)  # Process video to generate replies

    combined_prompt = " ".join(replies)  # Combine all replies into a single prompt
    generated_video_path = generate_video_from_prompt(combined_prompt)  # Generate video
    return f"Generated Prompt:\n{combined_prompt}", generated_video_path

# Define the architecture explanation
architecture_description = """
### System Architecture and Workflow

This system integrates multiple advanced AI components to generate a video based on textual and visual inputs. Here's how it works:

1. **Input Video Upload**:
   - The user uploads a video through the Gradio interface.
   - The video is analyzed frame-by-frame.

2. **Visual Language Model (VLM) Integration**:
   - For every 30th frame, a Visual Language Model (NVIDIA Neva) is invoked.
   - The VLM extracts information about the person in the frame (e.g., gender and expression) based on a pre-defined prompt.

3. **Prompt Aggregation**:
   - The responses from the VLM for all sampled frames are aggregated to form a consolidated text prompt.

4. **Diffusion-Based Video Generation**:
   - The consolidated prompt is passed to a text-to-video diffusion pipeline (`damo-vilab/text-to-video-ms-1.7b`).
   - The pipeline generates video frames based on the textual description.

5. **Output**:
   - The generated frames are exported as a video file.
   - The Gradio interface displays the generated prompt and the final video.

### Key Components:
- **NVIDIA Neva VLM**: Handles image-based understanding and generates descriptive text.
- **Diffusion Pipeline**: A generative model for creating videos from text descriptions.
- **Gradio**: Provides an interactive interface for the user to upload videos and view results.

### Example Use Case:
- A video showing a person's facial expressions (e.g., smiling or laughing) is uploaded.
- The system identifies the gender and expression and generates a new video depicting these characteristics.

This architecture can be extended to other domains involving multi-modal AI tasks.
"""

# Define the Gradio interface
with gr.Blocks() as interface:
    gr.Markdown("# Text-to-Video Prompt Processor")
    gr.Markdown(
        "Upload a video and analyze its content. The system will generate a new video based on the detected expressions."
    )
    gr.Markdown(architecture_description)  # Add the architecture explanation
    with gr.Row():
        with gr.Column():
            video_input = gr.Video(label="Upload a video file")
        with gr.Column():
            text_output = gr.Text(label="Generated Prompt")
            video_output = gr.Video(label="Generated Video")
    submit_button = gr.Button("Generate")
    submit_button.click(
        fn=gradio_process_input,
        inputs=[video_input],
        outputs=[text_output, video_output],
    )

# Launch the interface
if __name__ == "__main__":
    interface.launch(debug=True)

